# kubectl create -f job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  # Autogenerated name
  generateName: modeling-
  namespace: induction-labs
  labels:
    kueue.x-k8s.io/queue-name: local-queue # Assign to queue

spec:
  suspend: true # Important: Start suspended, Kueue will unsuspend
  template:
    spec:
      restartPolicy: Never # TODO: Set to never - use jobs
      runtimeClassName: nvidia
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      imagePullSecrets:
        - name: depot-registry-secret
        # - name: gcr-artifact-secret # Reference the secret
      containers:
        - name: modeling
          # image: us-central1-docker.pkg.dev/induction-labs/induction-docker/modeling:latest
          image: registry.depot.dev/v2tbx2d1w1:4x3d3ctfj7-remote
          imagePullPolicy: Always # Always pull latest
          command: ["devenv", "shell"]
          env:
            - name: LD_PRELOAD
              value: /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1
            - name: LD_LIBRARY_PATH
              value: /usr/local/cuda/lib64
            # - name: NCCL_DEBUG
            #   value: INFO
          args:
            - |
              mdl run exp_configs/action_instruct/qwen_25o/Qwen25OActionExperimentConfig_GPU/20250718_211225.toml -rhw # single gpu
            # mdl run exp_configs/action_instruct/qwen_25o/Qwen25OActionExperimentConfig_GPU/20250719_045722.toml -rhw
          resources:
            limits:
              memory: "128Gi"
              cpu: "32"
              nvidia.com/gpu: "8"
            requests:
              memory: "128Gi"
              cpu: "32"
              nvidia.com/gpu: "8"
          volumeMounts:
            - name: dev-shm
              mountPath: /dev/shm
      volumes:
        - name: dev-shm
          emptyDir:
            medium: Memory
            sizeLimit: 100Gi
