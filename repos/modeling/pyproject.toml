[project]
name = "modeling"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
authors = [{ name = "Jeffrey Li", email = "jeffrey.dot.li@gmail.com" }]
requires-python = ">=3.12"
dependencies = [
  "datasets>=3.6.0",
  "numpy>=2.2.6",
  "ray>=2.45.0",
  "torch==2.7.0",
  "torchvision>=0.22.0",
  "torchaudio>=2.7.0",
  "triton >=3.3.0",
  "tqdm>=4.67.1",
  "transformers>=4.52.4",
  "wandb>=0.19.11",
  "accelerate>=1.7.0",
  "pillow>=11.2.1",
]

[dependency-groups]
dev = ["ipython>=9.3.0", "pytest>=8.3.5"]
# vllm = ["vllm"]

[tool.uv]
default-groups = ["dev"]
# TODO: figure out how to only support linux aarch64/x86, and ban windows + darwin
environments = ["platform_machine == 'x86_64'", "platform_machine == 'aarch64'"]

[tool.uv.sources]
torch = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
# We need to tell uv to specifically use the torch index of triton instead of pypi or it won't find the arm64 version
triton = [{ index = "torch_general" }]
torchvision = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
torchaudio = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
# TODO: Figure out hwo to get flash_attention_2 working.
# vllm = { path = "../vllm" }

[[tool.uv.index]]
name = "torch_general"
url = "https://download.pytorch.org/whl"

[[tool.uv.index]]
name = "torch_12.8"
url = "https://download.pytorch.org/whl/cu128"
explicit = true


[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]
sources = ["src"]
