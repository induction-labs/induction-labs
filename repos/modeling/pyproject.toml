[project]
name = "modeling"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
authors = [{ name = "Jeffrey Li", email = "jeffrey.dot.li@gmail.com" }]
requires-python = ">=3.12"
dependencies = [
  "datasets>=3.6.0",
  "numpy>=2.2.6",
  "ray[client]>=2.47.1",
  "torch==2.7.1",
  "torchvision>=0.22.1",
  "torchaudio>=2.7.1",
  "triton >=3.3.1",
  "tqdm>=4.67.1",
  "transformers>=4.52.4",
  "wandb>=0.19.11",
  "accelerate>=1.7.0",
  "pillow>=11.2.1",
  "pydantic>=2.11.5",
  "tomli",
  "tomli-w>=1.2.0",
  "typer>=0.16.0",
  "einops>=0.8.1",
  "synapse",
]
[project.scripts]
mdl = "modeling.main:app"


[dependency-groups]
dev = [
  "huggingface-hub[cli]>=0.32.4",
  "ipython>=9.3.0",
  "matplotlib>=3.10.3",
  "pytest>=8.3.5",
  "seaborn>=0.13.2",
  "tabulate>=0.9.0",
  "ipykernel>=6.29.5",
  "ipynb>=0.5.1",
  "jupyter>=1.1.1",
  "pyzmq>=27.0.0",
  "tensorboard>=2.19.0",
  "tensorboard-plugin-profile>=2.20.1",
  "torch-tb-profiler>=0.4.3",
  "expecttest>=0.3.0",
]
# TODO: Validate that this is always correct version of torch + other deps
flash-attn = [
  "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3+cu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and platform_machine == 'x86_64'",
]
head = ["ray[default,client]"]
[tool.uv]
default-groups = ["dev", "head"]

# Needs to be in workspace root
# https://github.com/pytorch/pytorch/issues/153401#issuecomment-2881445494
# override-dependencies = ["nvidia-nccl-cu12==2.26.5"]


# TODO: figure out how to only support linux aarch64/x86, and ban windows + darwin

[tool.uv.sources]
torch = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
# We need to tell uv to specifically use the torch index of triton instead of pypi or it won't find the arm64 version
triton = [{ index = "torch_general" }]
torchvision = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
torchaudio = [{ index = "torch_12.8", marker = "sys_platform == 'linux'" }]
synapse = { workspace = true }
tomli-w = { git = "ssh://git@github.com/induction-labs/tomli-w-none.git" }
tomli = { git = "ssh://git@github.com/induction-labs/tomli-none.git" }
transformers = { git = "ssh://git@github.com/induction-labs/transformers.git", branch = "fix-qwen2vl-positionid-mask" }

[[tool.uv.index]]
name = "torch_general"
url = "https://download.pytorch.org/whl"

[[tool.uv.index]]
name = "torch_12.8"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.ruff]
extend-exclude = ["tests/flash_attention.py"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]
sources = ["src"]
environments = ["platform_machine == 'x86_64'", "platform_machine == 'aarch64'"]
