{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6d6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def prepare_attention_mask(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        # device: torch.device,\n",
    "        min_dtype: float = -1.0,\n",
    "        # cache_position: torch.Tensor,\n",
    "        batch_size: int =1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n",
    "        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n",
    "\n",
    "        Args:\n",
    "            attention_mask (`torch.Tensor`):\n",
    "                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n",
    "            sequence_length (`int`):\n",
    "                The sequence length being processed.\n",
    "            target_length (`int`):\n",
    "                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n",
    "            dtype (`torch.dtype`):\n",
    "                The dtype to use for the 4D attention mask.\n",
    "            device (`torch.device`):\n",
    "                The device to place the 4D attention mask on.\n",
    "            min_dtype (`float`):\n",
    "                The minimum value representable with the dtype `dtype`.\n",
    "            cache_position (`torch.Tensor`):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence.\n",
    "            batch_size (`torch.Tensor`):\n",
    "                Batch size.\n",
    "        \"\"\"\n",
    "        \n",
    "        causal_mask = torch.full(\n",
    "            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, \n",
    "        )\n",
    "        if sequence_length != 1:\n",
    "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            # diagonal=1 excludes the main diagonal, \n",
    "            # so bottom left triangle (including main) now is filled with 0,\n",
    "            # top right (excluding main) is filled with min_dtype\n",
    "        # causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            \n",
    "            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "            padding_mask = padding_mask == 0\n",
    "            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "\n",
    "        return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238855e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0., -1., -1., -1., -1.],\n",
      "          [ 0.,  0., -1., -1., -1.],\n",
      "          [ 0.,  0.,  0., -1., -1.],\n",
      "          [ 0.,  0.,  0., -1., -1.],\n",
      "          [ 0.,  0.,  0., -1.,  0.]]]])\n"
     ]
    }
   ],
   "source": [
    "s = [1,1,1,0,1]\n",
    "seq_length =len(s)\n",
    "\n",
    "attention_mask = torch.tensor(s).reshape(1, seq_length)\n",
    "mask4d = prepare_attention_mask(\n",
    "    attention_mask=attention_mask,\n",
    "    sequence_length=seq_length,\n",
    "    target_length=seq_length,   \n",
    ")\n",
    "print(mask4d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
