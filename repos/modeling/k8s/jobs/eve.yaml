# kubectl create -f eve.yaml
apiVersion: batch/v1
kind: Job
metadata:
  # Autogenerated name
  generateName: eve-
  namespace: induction-labs
  labels:
    kueue.x-k8s.io/queue-name: local-queue # Assign to queue

spec:
  suspend: true # Important: Start suspended, Kueue will unsuspend
  backoffLimit: 0 # No retries
  ttlSecondsAfterFinished: 172800 # 2 days
  template:
    spec:
      restartPolicy: Never # TODO: Set to never - use jobs
      runtimeClassName: nvidia
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      imagePullSecrets:
        - name: depot-registry-secret
      containers:
        - name: vllm-server
          # image: <PLACEHOLDER>
          image: registry.depot.dev/v2tbx2d1w1:959l8d0891-eve
          imagePullPolicy: IfNotPresent
          # Don't specify command, use default command in Dockerfile
          env:
            - name: LD_PRELOAD
              value: /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1
            - name: LD_LIBRARY_PATH
              value: /usr/local/cuda/lib64
          args:
            [
              "eve",
              "vllm",
              "start",
              "gs://induction-labs/checkpoints/uitars_sft_7b_yehaw_good_nice/2025-07-31T05-18-29.nTtAsFOt/step_-1",
              "--num-gpus=8",
            ]
          resources:
            limits:
              memory: "128Gi"
              cpu: "32"
              nvidia.com/gpu: "8"
            requests:
              memory: "128Gi"
              cpu: "32"
              nvidia.com/gpu: "8"
          volumeMounts:
            - name: dev-shm
              mountPath: /dev/shm
      volumes:
        - name: dev-shm
          emptyDir:
            medium: Memory
